---
title: "Your Title"
subtitle: "BMIN503/EPID600 Final Project"
author: "FirstName LastName"
format: html
editor: visual
number-sections: true
embed-resources: true
---

------------------------------------------------------------------------

Use this template to complete your project throughout the course. Your Final Project presentation will be based on the contents of this document. Replace the title/name above and text below with your own, but keep the headers. Feel free to change the theme and other display settings, although this is not required. I added a new sentence

## Overview {#sec-overview}

The aim of this project is to develop a machine learning-based lung cancer detection system that assists physicians in identifying potentially malignant nodules by analyzing CT scan images, thereby improving diagnostic accuracy and efficiency. The core objectives of the project are to reduce the false-positive rate, decrease physicians' workload, and provide support for early lung cancer diagnosis.

## Introduction {#sec-introduction}

Lung cancer is one of the leading causes of cancer-related deaths globally and is usually detected at an advanced stage, resulting in limited treatment options. Early detection can significantly improve survival rates, but current diagnostic methods rely on extensive manual review of CT scan images by radiologists, a process that is both time-consuming and susceptible to subjective factors. The aim of this project is to develop a machine learning model to aid in the early diagnosis of lung cancer, with the goal of reducing false positives and improving diagnostic efficiency, thereby supporting clinicians. The project is inherently interdisciplinary, incorporating knowledge from oncology and machine learning. Oncologists provided insights into cancer progression, risk factors, and diagnostic challenges, while machine learning experts provided methods for model selection, data enhancement, and multimodal data integration.

I spoke with Dr. Mowery and Dr. Fan, who emphasized the importance of reducing the false-positive rate and provided clinical knowledge about key risk factors to help optimize early lung cancer diagnosis, and Dr. Mowery, who suggested integrating image data and clinical text annotations and proposed technical ideas for combining visual and non-visual data to improve prediction accuracy. I combined their claims of having made predictions with data and evaluated the predictions.

## Methods {#sec-methods}

Describe the data used and general methodological approach used to address the problem described in the @sec-introduction. Subsequently, incorporate full R code necessary to retrieve and clean data, and perform analysis. Be sure to include a description of code so that others (including your future self) can understand what you are doing and why.

```{r}
library(dplyr)
library(ggplot2)
library(readr)

options(warn = -1)

df <- read_csv('survey lung cancer.csv')

print(df)
```

**Note: In this dataset, YES=2 & NO=1**

```{r}
dim(df)

sum(duplicated(df))
df <- df[!duplicated(df), ]
colSums(is.na(df))
```

```{r}
str(df)
```

```{r}
summary(df)
```

```{r}
names(df) <- trimws(names(df))
print(names(df))
```

```{r}
df$GENDER <- as.numeric(factor(df$GENDER)) - 1
df$LUNG_CANCER <- as.numeric(factor(df$LUNG_CANCER)) - 1
df$SMOKING <- as.numeric(factor(df$SMOKING)) - 1
df$YELLOW_FINGERS <- as.numeric(factor(df$YELLOW_FINGERS)) - 1
df$ANXIETY <- as.numeric(factor(df$ANXIETY)) - 1
df$PEER_PRESSURE <- as.numeric(factor(df$PEER_PRESSURE)) - 1
df$`CHRONIC DISEASE` <- as.numeric(factor(df$`CHRONIC DISEASE`)) - 1
df$FATIGUE <- as.numeric(factor(df$FATIGUE)) - 1
df$ALLERGY <- as.numeric(factor(df$ALLERGY)) - 1
df$WHEEZING <- as.numeric(factor(df$WHEEZING)) - 1
df$`ALCOHOL CONSUMING` <- as.numeric(factor(df$`ALCOHOL CONSUMING`)) - 1
df$COUGHING <- as.numeric(factor(df$COUGHING)) - 1
df$`SHORTNESS OF BREATH` <- as.numeric(factor(df$`SHORTNESS OF BREATH`)) - 1
df$`SWALLOWING DIFFICULTY` <- as.numeric(factor(df$`SWALLOWING DIFFICULTY`)) - 1
df$`CHEST PAIN` <- as.numeric(factor(df$`CHEST PAIN`)) - 1

df
```

```{r}
str(df)
```

```{r}
ggplot(df, aes(x = as.factor(LUNG_CANCER), fill = as.factor(LUNG_CANCER))) +
  geom_bar() +
  scale_fill_manual(values = c("#0073C2FF", "#EFC000FF")) +
  labs(title = "Target Distribution", x = "LUNG_CANCER", y = "Count") +
  theme_minimal() +
  theme(legend.position = "none")
```

```{r}
table(df$LUNG_CANCER)

plot_distribution <- function(col_name, df) {
  library(ggplot2)
  
  ggplot(df, aes(x = as.factor(get(col_name)), fill = as.factor(LUNG_CANCER))) +
    geom_bar(position = "fill", stat = "count") +
    scale_fill_manual(values = c("#0073C2FF", "#EFC000FF")) +
    labs(
      title = paste("Distribution of LUNG_CANCER by", col_name),
      x = col_name,
      y = "Proportion"
    ) +
    theme_minimal()
}

plot_distribution("GENDER", df)
plot_distribution("AGE", df)
plot_distribution("SMOKING", df)
plot_distribution("YELLOW_FINGERS", df)
plot_distribution("ANXIETY", df)
plot_distribution("PEER_PRESSURE", df)
plot_distribution("CHRONIC DISEASE", df)
plot_distribution("FATIGUE", df)
plot_distribution("ALLERGY", df)
plot_distribution("WHEEZING", df)
plot_distribution("ALCOHOL CONSUMING", df)
plot_distribution("COUGHING", df)
plot_distribution("SHORTNESS OF BREATH", df)
plot_distribution("SWALLOWING DIFFICULTY", df)
plot_distribution("CHEST PAIN", df)
# Based on the visualizations, it is evident that the features GENDER, AGE, SMOKING, and SHORTNESS OF BREATH show minimal correlation with LUNG CANCER in the dataset. Therefore, we can remove these features to streamline and refine the dataset.
```

```{r}
df_new <- df %>% select(-GENDER, -AGE, -SMOKING, -`SHORTNESS OF BREATH`)
names(df_new) <- gsub(" ", "_", names(df_new))

print(names(df_new))

print(df_new)

```

```{r}
library(ggplot2)
library(reshape2)
library(corrplot)

df_numeric <- df_new[, sapply(df_new, is.numeric)]

cn <- cor(df_numeric, use = "complete.obs")

print(cn)

corrplot(cn, method = "color", col = colorRampPalette(c("blue", "white", "red"))(200),
         type = "full", 
         addCoef.col = "black",
         number.cex = 0.4,
         tl.cex = 0.2,
         tl.col = "black")
```

YELLOW_FINGERS and ANXIETY have a correlation of 0.56, showing a strong positive correlation. None of the other positive correlations are as high. For example, FATIGUE and CHRONIC_DISEASE have a correlation of -0.10, which is hardly linear. Most of the correlations between the variables are close to 0, which suggests that the variables may be independent or have a non-linear relationship.

#### Linear Regression

```{r}
library(caret)
library(ROSE)
library(pROC)
library(rpart.plot)

# Create a new variable and convert the target variable to a factor
df_new$ANXYELFIN <- df_new$ANXIETY * df_new$YELLOW_FINGERS
X <- df_new[, !(names(df_new) %in% c("LUNG_CANCER"))]
y <- df_new$LUNG_CANCER
y <- factor(y, levels = c(0, 1))

# Data set partitioning (50% training set, 25% validation set, 25% test set)
set.seed(1234)
train_index <- createDataPartition(y, p = 0.5, list = FALSE)
X_train <- X[train_index, ]
y_train <- y[train_index]
X_temp <- X[-train_index, ]
y_temp <- y[-train_index]

set.seed(1234)
val_test_index <- createDataPartition(y_temp, p = 0.5, list = FALSE)
X_val <- X_temp[val_test_index, ]
y_val <- y_temp[val_test_index]
X_test <- X_temp[-val_test_index, ]
y_test <- y_temp[-val_test_index]

cat("\nDataset Sizes Before Resampling:\n")
cat("Training set size: ", dim(X_train)[1], "\n")
cat("Validation set size: ", dim(X_val)[1], "\n")
cat("Test set size: ", dim(X_test)[1], "\n")

# Merge training data into a single data frame to accommodate resampling
train_data <- data.frame(y_train, X_train)

# Balancing training data
cat("\nApplying ROSE Oversampling...\n")
train_data_rose <- ovun.sample(y_train ~ ., data = train_data, method = "both", p = 0.5, N = 1000, seed = 42)$data

# Checking data distribution after resampling
cat("\nClass Distribution After Resampling:\n")
print(table(train_data_rose$y_train))

# Re-split into x and y
X_train_rose <- train_data_rose[, -1]
y_train_rose <- train_data_rose$y_train

# K-Fold cross-validation parameters
set.seed(1234)
train_control <- trainControl(method = "cv", number = 5, verboseIter = TRUE)

```

```{r}
library(caret)
library(pROC)
library(e1071)  # SVM
library(randomForest)
library(class)  # KNN
# Check column names after resampling and dynamically get target variable names
target_var <- colnames(train_data_rose)[1]  # Dynamically get the target variable column name

# Perform K-Fold cross-validation
cat("\nPerforming K-Fold Cross Validation on Training Set...\n")
lr_model_cv <- train(
  as.formula(paste(target_var, "~ .")),  # Dynamic construction of formulas
  data = train_data_rose,
  method = "glm",
  family = binomial,
  trControl = train_control
)

# Output K-Fold cross-validation results
cat("\nCross-Validation Results:\n")
print(lr_model_cv)

# Final model trained with K-Fold
lr_model <- lr_model_cv$finalModel

# Validation set prediction
y_val_prob <- predict(lr_model, newdata = data.frame(X_val), type = "response")
y_val_pred <- ifelse(y_val_prob > 0.5, 1, 0)

# Validation Set Performance Evaluation
confusion_matrix_val <- confusionMatrix(as.factor(y_val_pred), factor(y_val, levels = c(0, 1)))
val_accuracy <- confusion_matrix_val$overall["Accuracy"]
cat("\nValidation Set Accuracy:\n")
print(val_accuracy)

# Test Set Prediction
y_test_prob <- predict(lr_model, newdata = data.frame(X_test), type = "response")
y_test_pred <- ifelse(y_test_prob > 0.5, 1, 0)

# Test Set Performance Evaluation
confusion_matrix_test <- confusionMatrix(as.factor(y_test_pred), factor(y_test, levels = c(0, 1)))
test_accuracy <- confusion_matrix_test$overall["Accuracy"]
cat("\nTest Set Accuracy:\n")
print(test_accuracy)

# Plotting ROC curves and calculating AUC
cat("\nPlotting ROC Curve for Test Set...\n")

# Test set ROC curve
roc_test <- roc(as.numeric(y_test) - 1, y_test_prob, smooth = TRUE)  # Adding Smoothing Parameters
auc_test <- auc(roc_test)

# roc plot
plot(
  roc_test,
  col = "red",
  main = "ROC Curve for Test Set",
  print.auc = TRUE,
  legacy.axes = TRUE,
  print.auc.y = 0.4
)

# auc result
cat("\nTest Set AUC:\n")
print(auc_test)

```

Decision Tree Model

```{r}
# Training Decision Tree Models
cat("\nTraining Decision Tree with K-Fold Cross Validation on Resampled Data...\n")

tree_model_cv <- train(
  as.formula(paste(target_var, "~ .")), 
  data = train_data_rose,
  method = "rpart",
  trControl = train_control,
  tuneGrid = expand.grid(cp = seq(0.01, 0.1, by = 0.01))  # 调整复杂度参数
)
cat("\nDecision Tree Cross-Validation Results:\n")
print(tree_model_cv)

tree_model <- tree_model_cv$finalModel

cat("\nPlotting Decision Tree...\n")
rpart.plot(tree_model)

y_val_prob_tree <- predict(tree_model, newdata = data.frame(X_val), type = "prob")[, 2]
y_val_pred_tree <- ifelse(y_val_prob_tree > 0.5, 1, 0)

confusion_matrix_val_tree <- confusionMatrix(as.factor(y_val_pred_tree), factor(y_val, levels = c(0, 1)))
val_accuracy_tree <- confusion_matrix_val_tree$overall["Accuracy"]
cat("\nValidation Set Accuracy (Decision Tree):\n")
print(val_accuracy_tree)

y_test_prob_tree <- predict(tree_model, newdata = data.frame(X_test), type = "prob")[, 2]
y_test_pred_tree <- ifelse(y_test_prob_tree > 0.5, 1, 0)

confusion_matrix_test_tree <- confusionMatrix(as.factor(y_test_pred_tree), factor(y_test, levels = c(0, 1)))
test_accuracy_tree <- confusion_matrix_test_tree$overall["Accuracy"]
cat("\nTest Set Accuracy (Decision Tree):\n")
print(test_accuracy_tree)

roc_test_tree <- roc(as.numeric(y_test) - 1, y_test_prob_tree, smooth = TRUE)
auc_test_tree <- auc(roc_test_tree)
plot(
  roc_test_tree,
  col = "blue",
  main = "ROC Curve for Decision Tree (Resampled Data)",
  print.auc = TRUE,
  legacy.axes = TRUE,
  print.auc.y = 0.4
)
cat("\nTest Set AUC (Decision Tree):\n")
print(auc_test_tree)

```

Random Forest

```{r}
cat("\nTraining Random Forest with K-Fold Cross Validation...\n")
rf_model_cv <- train(
  as.formula(paste(target_var, "~ .")),
  data = train_data_rose,
  method = "rf",
  trControl = train_control,
  tuneGrid = expand.grid(mtry = sqrt(ncol(X_train_rose)))
)

cat("\nRandom Forest Cross-Validation Results:\n")
print(rf_model_cv)

rf_model <- rf_model_cv$finalModel

y_test_prob_rf <- predict(rf_model, newdata = data.frame(X_test), type = "prob")[, 2]
y_test_pred_rf <- ifelse(y_test_prob_rf > 0.5, 1, 0)

confusion_matrix_test_rf <- confusionMatrix(as.factor(y_test_pred_rf), factor(y_test, levels = c(0, 1)))
cat("\nTest Set Accuracy (Random Forest):\n")
print(confusion_matrix_test_rf$overall["Accuracy"])

roc_test_rf <- roc(as.numeric(y_test) - 1, y_test_prob_rf, smooth = TRUE)
auc_test_rf <- auc(roc_test_rf)
plot(
  roc_test_rf,
  col = "green",
  main = "ROC Curve for Random Forest",
  print.auc = TRUE,
  legacy.axes = TRUE,
  print.auc.y = 0.4
)
cat("\nTest Set AUC (Random Forest):\n")
print(auc_test_rf)
```

#### **Support Vector Classifier (SVC)**

```{r}
library(kernlab)

cat("\nTraining Support Vector Classifier (SVC) with K-Fold Cross Validation...\n")
svc_model <- ksvm(
  x = as.matrix(X_train_rose),
  y = as.factor(y_train_rose),
  type = "C-svc",
  kernel = "rbfdot",
  C = 1,
  prob.model = TRUE
)

cat("\nSVC Model Trained Successfully.\n")

# Test Set Evaluation
# Probability values for prediction categories
y_test_prob_svc <- predict(svc_model, newdata = as.matrix(X_test), type = "probabilities")[, 2]
y_test_pred_svc <- ifelse(y_test_prob_svc > 0.5, 1, 0)

confusion_matrix_test_svc <- confusionMatrix(as.factor(y_test_pred_svc), factor(y_test, levels = c(0, 1)))
cat("\nTest Set Accuracy (SVC):\n")
print(confusion_matrix_test_svc$overall["Accuracy"])

roc_test_svc <- roc(as.numeric(y_test) - 1, y_test_prob_svc, smooth = TRUE)
auc_test_svc <- auc(roc_test_svc)
plot(
  roc_test_svc,
  col = "blue",
  main = "ROC Curve for SVC",
  print.auc = TRUE,
  legacy.axes = TRUE,
  print.auc.y = 0.4
)
cat("\nTest Set AUC (SVC):\n")
print(auc_test_svc)
```

#### Gaussian Naive Bayes

```{r}
library(naivebayes)

cat("\nTraining Gaussian Naive Bayes...\n")
gnb_model_cv <- train(
  as.formula(paste(target_var, "~ .")),
  data = train_data_rose,
  method = "naive_bayes",
  trControl = train_control
)

cat("\nGaussian Naive Bayes Cross-Validation Results:\n")
print(gnb_model_cv)

gnb_model <- gnb_model_cv$finalModel

y_test_prob_gnb <- predict(gnb_model, newdata = data.frame(X_test), type = "prob")[, 2]
y_test_pred_gnb <- ifelse(y_test_prob_gnb > 0.5, 1, 0)

confusion_matrix_test_gnb <- confusionMatrix(as.factor(y_test_pred_gnb), factor(y_test, levels = c(0, 1)))
cat("\nTest Set Accuracy (Gaussian Naive Bayes):\n")
print(confusion_matrix_test_gnb$overall["Accuracy"])

roc_test_gnb <- roc(as.numeric(y_test) - 1, y_test_prob_gnb, smooth = TRUE)
auc_test_gnb <- auc(roc_test_gnb)
plot(
  roc_test_gnb,
  col = "purple",
  main = "ROC Curve for Gaussian Naive Bayes",
  print.auc = TRUE,
  legacy.axes = TRUE,
  print.auc.y = 0.4
)
cat("\nTest Set AUC (Gaussian Naive Bayes):\n")
print(auc_test_gnb)

```

#### K Nearest Neighbor

```{r}
cat("\nTraining K-Nearest Neighbor (KNN) with K-Fold Cross Validation...\n")
knn_model_cv <- train(
  as.formula(paste(target_var, "~ .")),
  data = train_data_rose,
  method = "knn",
  trControl = train_control,
  tuneGrid = expand.grid(k = seq(3, 15, by = 2))
)

cat("\nKNN Cross-Validation Results:\n")
print(knn_model_cv)

knn_model <- knn_model_cv$finalModel

y_test_prob_knn <- predict(knn_model, newdata = data.frame(X_test), type = "prob")[, 2]
y_test_pred_knn <- ifelse(y_test_prob_knn > 0.5, 1, 0)

confusion_matrix_test_knn <- confusionMatrix(as.factor(y_test_pred_knn), factor(y_test, levels = c(0, 1)))
cat("\nTest Set Accuracy (KNN):\n")
print(confusion_matrix_test_knn$overall["Accuracy"])

# 绘制ROC曲线
roc_test_knn <- roc(as.numeric(y_test) - 1, y_test_prob_knn, smooth = TRUE)
auc_test_knn <- auc(roc_test_knn)
plot(
  roc_test_knn,
  col = "orange",
  main = "ROC Curve for KNN",
  print.auc = TRUE,
  legacy.axes = TRUE,
  print.auc.y = 0.4
)
cat("\nTest Set AUC (KNN):\n")
print(auc_test_knn)
```

```{r}
cat("\nTraining XGBoost with K-Fold Cross Validation...\n")
xgb_model_cv <- train(
  as.formula(paste(target_var, "~ .")),
  data = train_data_rose,
  method = "xgbTree",
  trControl = train_control,
  tuneGrid = expand.grid(nrounds = 100, max_depth = 6, eta = 0.3, gamma = 0, colsample_bytree = 1, min_child_weight = 1, subsample = 1)
)

cat("\nXGBoost Cross-Validation Results:\n")
print(xgb_model_cv)

xgb_model <- xgb_model_cv$finalModel

# Test Set Evaluation
# Return the probability vector
y_test_prob_xgb <- predict(xgb_model, newdata = as.matrix(X_test))  
# II. Forecasts by category
y_test_pred_xgb <- ifelse(y_test_prob_xgb > 0.5, 1, 0)  

# Test Set Performance Evaluation
confusion_matrix_test_xgb <- confusionMatrix(as.factor(y_test_pred_xgb), factor(y_test, levels = c(0, 1)))
cat("\nTest Set Accuracy (XGBoost):\n")
print(confusion_matrix_test_xgb$overall["Accuracy"])

roc_test_xgb <- roc(as.numeric(y_test) - 1, y_test_prob_xgb, smooth = TRUE)
auc_test_xgb <- auc(roc_test_xgb)
plot(
  roc_test_xgb,
  col = "red",
  main = "ROC Curve for XGBoost",
  print.auc = TRUE,
  legacy.axes = TRUE,
  print.auc.y = 0.4
)
cat("\nTest Set AUC (XGBoost):\n")
print(auc_test_xgb)
```

ROC Curve

```{r}
library(pROC)
library(ggplot2)

# Collect ROC objects for all models
roc_list <- list(
  # Logistic Regression ROC
  "Logistic Regression" = roc_test,
  # Decision Tree ROC
  "Decision Tree" = roc_test_tree,
  # Random Forest ROC
  "Random Forest" = roc_test_rf,
  # Support Vector Classifier ROC
  "SVC" = roc_test_svc,
  # Gaussian Naive Bayes ROC
  "Gaussian Naive Bayes" = roc_test_gnb,
  # K-Nearest Neighbor ROC
  "KNN" = roc_test_knn,
  # XGBoost ROC
  "XGBoost" = roc_test_xgb
)

# Converting ROC objects to dataframe format
roc_data <- lapply(names(roc_list), function(model_name) {
  roc_obj <- roc_list[[model_name]]
  data.frame(
    FPR = 1 - roc_obj$specificities,  # False Positive Rate
    TPR = roc_obj$sensitivities,      # True Positive Rate
    Model = model_name
  )
})

# Merge all model data
roc_data <- do.call(rbind, roc_data) 

# Plotting ROC curves for multiple models
ggplot(roc_data, aes(x = FPR, y = TPR, color = Model)) +
  geom_line(size = 0.5) +
  scale_color_manual(
    values = c(
      "Logistic Regression" = "red",
      "Decision Tree" = "blue",
      "Random Forest" = "green",
      "SVC" = "purple",
      "Gaussian Naive Bayes" = "yellow",
      "KNN" = "orange",
      "XGBoost" = "cyan"
    )
  ) +
  labs(
    title = "ROC Curves for Multiple Models",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")

```

## Results {#sec-results}

Describe your results and include relevant tables, plots, and code/comments used to obtain them. You may refer to the @sec-methods as needed. End with a brief conclusion of your findings related to the question you set out to address. You can include references if you'd like, but this is not required.

## Conclusion

This the conclusion. The @sec-results can be invoked here.
