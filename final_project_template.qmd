---
title: "Comparison of machine learning-based models for early diagnosis and prediction of lung cancer"
subtitle: "BMIN503/EPID600 Final Project"
author: "FirstName LastName"
format: html
editor: visual
number-sections: true
embed-resources: true
---

------------------------------------------------------------------------

Use this template to complete your project throughout the course. Your Final Project presentation will be based on the contents of this document. Replace the title/name above and text below with your own, but keep the headers. Feel free to change the theme and other display settings, although this is not required. I added a new sentence

## Overview {#sec-overview}

The aim of this project is to develop a machine learning-based lung cancer detection system that assists physicians in identifying potentially malignant nodules by analyzing CT scan images, thereby improving diagnostic accuracy and efficiency. The core objectives of the project are to reduce the false-positive rate, decrease physicians' workload, and provide support for early lung cancer diagnosis.

## Introduction {#sec-introduction}

Lung cancer is one of the leading causes of cancer-related deaths globally and is often diagnosed at an advanced stage, limiting treatment options and reducing survival rates. Early detection plays a crucial role in improving patient outcomes, but current diagnostic practices primarily rely on subjective assessments and are prone to inefficiencies. The aim of this project is to develop a machine learning-based predictive model for lung cancer diagnosis, using patient data to identify key risk factors and improve diagnostic accuracy. This data-driven approach focuses on reducing false positives and enhancing clinical decision-making efficiency, ultimately supporting clinicians in early detection efforts.

Collaborations with Dr. Mowery and Dr. Fan were instrumental in shaping this project. Dr. Fan provided critical insights into the clinical presentation, risk factors, and diagnostic challenges associated with lung cancer, while Dr. Mowery highlighted the importance of minimizing false-positive rates to ensure actionable results. This project leverages advanced machine learning techniques, informed by their guidance, to analyze structured data and evaluate predictive performance. By focusing solely on patient data, the study demonstrates the potential of machine learning in streamlining lung cancer diagnosis and supporting evidence-based clinical practices.

## Methods {#sec-methods}

The analysis focuses on a dataset (survey lung cancer.csv) with information about individuals, including factors potentially influencing lung cancer incidence. Each row represents a respondent, and columns capture variables like smoking habits, gender, symptoms, and diagnoses.

The primary objective is to predict the presence of lung cancer (LUNG_CANCER), a binary outcome. A robust methodological approach ensures the dataset is cleaned, analyzed, and modeled effectively. The steps include:

Data Loading and Cleaning: Eliminate duplicates, encode categorical data numerically, and address imbalances. Exploratory Data Analysis (EDA): Analyze distributions and relationships between variables to identify significant predictors. Feature Engineering: Create new features based on domain knowledge to enhance model performance. Model Development: Train multiple machine learning models (e.g., Logistic Regression, Decision Tree, Random Forest, SVM, XGBoost) using K-Fold Cross-Validation for robust evaluation. Model Evaluation: Use accuracy, ROC curves, and AUC as performance metrics to compare models.

```{r}
library(dplyr)
library(ggplot2)
library(readr)

options(warn = -1)

df <- read_csv('survey lung cancer.csv')

print(df)
```

Note: In this dataset, YES=2 & NO=1

```{r}
dim(df)

sum(duplicated(df))
df <- df[!duplicated(df), ]
colSums(is.na(df))
```

```{r}
str(df)
```

```{r}
summary(df)
```

```{r}
names(df) <- trimws(names(df))
print(names(df))
```

```{r}
df$GENDER <- as.numeric(factor(df$GENDER)) - 1
df$LUNG_CANCER <- as.numeric(factor(df$LUNG_CANCER)) - 1
df$SMOKING <- as.numeric(factor(df$SMOKING)) - 1
df$YELLOW_FINGERS <- as.numeric(factor(df$YELLOW_FINGERS)) - 1
df$ANXIETY <- as.numeric(factor(df$ANXIETY)) - 1
df$PEER_PRESSURE <- as.numeric(factor(df$PEER_PRESSURE)) - 1
df$`CHRONIC DISEASE` <- as.numeric(factor(df$`CHRONIC DISEASE`)) - 1
df$FATIGUE <- as.numeric(factor(df$FATIGUE)) - 1
df$ALLERGY <- as.numeric(factor(df$ALLERGY)) - 1
df$WHEEZING <- as.numeric(factor(df$WHEEZING)) - 1
df$`ALCOHOL CONSUMING` <- as.numeric(factor(df$`ALCOHOL CONSUMING`)) - 1
df$COUGHING <- as.numeric(factor(df$COUGHING)) - 1
df$`SHORTNESS OF BREATH` <- as.numeric(factor(df$`SHORTNESS OF BREATH`)) - 1
df$`SWALLOWING DIFFICULTY` <- as.numeric(factor(df$`SWALLOWING DIFFICULTY`)) - 1
df$`CHEST PAIN` <- as.numeric(factor(df$`CHEST PAIN`)) - 1

df
```

```{r}
str(df)
```

```{r}
ggplot(df, aes(x = as.factor(LUNG_CANCER), fill = as.factor(LUNG_CANCER))) +
  geom_bar() +
  scale_fill_manual(values = c("#0073C2FF", "#EFC000FF")) +
  labs(title = "Target Distribution", x = "LUNG_CANCER", y = "Count") +
  theme_minimal() +
  theme(legend.position = "none")
```

```{r}
table(df$LUNG_CANCER)

plot_distribution <- function(col_name, df) {
  library(ggplot2)
  
  ggplot(df, aes(x = as.factor(get(col_name)), fill = as.factor(LUNG_CANCER))) +
    geom_bar(position = "fill", stat = "count") +
    scale_fill_manual(values = c("#0073C2FF", "#EFC000FF")) +
    labs(
      title = paste("Distribution of LUNG_CANCER by", col_name),
      x = col_name,
      y = "Proportion"
    ) +
    theme_minimal()
}

plot_distribution("GENDER", df)
plot_distribution("AGE", df)
plot_distribution("SMOKING", df)
plot_distribution("YELLOW_FINGERS", df)
plot_distribution("ANXIETY", df)
plot_distribution("PEER_PRESSURE", df)
plot_distribution("CHRONIC DISEASE", df)
plot_distribution("FATIGUE", df)
plot_distribution("ALLERGY", df)
plot_distribution("WHEEZING", df)
plot_distribution("ALCOHOL CONSUMING", df)
plot_distribution("COUGHING", df)
plot_distribution("SHORTNESS OF BREATH", df)
plot_distribution("SWALLOWING DIFFICULTY", df)
plot_distribution("CHEST PAIN", df)
# Based on the visualizations, it is evident that the features GENDER, AGE, SMOKING, and SHORTNESS OF BREATH show minimal correlation with LUNG CANCER in the dataset. Therefore, we can remove these features to streamline and refine the dataset.
```

```{r}
df_new <- df %>% select(-GENDER, -AGE, -SMOKING, -`SHORTNESS OF BREATH`)
names(df_new) <- gsub(" ", "_", names(df_new))

print(names(df_new))

print(df_new)

```

```{r}
library(ggplot2)
library(reshape2)
library(corrplot)

df_numeric <- df_new[, sapply(df_new, is.numeric)]

cn <- cor(df_numeric, use = "complete.obs")

print(cn)

corrplot(cn, method = "color", col = colorRampPalette(c("blue", "white", "red"))(200),
         type = "full", 
         addCoef.col = "black",
         number.cex = 0.4,
         tl.cex = 0.2,
         tl.col = "black")
```

YELLOW_FINGERS and ANXIETY have a correlation of 0.56, showing a strong positive correlation. None of the other positive correlations are as high. For example, FATIGUE and CHRONIC_DISEASE have a correlation of -0.10, which is hardly linear. Most of the correlations between the variables are close to 0, which suggests that the variables may be independent or have a non-linear relationship.

```{r}
library(caret)
library(ROSE)
library(pROC)
library(rpart.plot)

# Create a new variable and convert the target variable to a factor
df_new$ANXYELFIN <- df_new$ANXIETY * df_new$YELLOW_FINGERS
X <- df_new[, !(names(df_new) %in% c("LUNG_CANCER"))]
y <- df_new$LUNG_CANCER
y <- factor(y, levels = c(0, 1))

# Data set partitioning (50% training set, 25% validation set, 25% test set)
set.seed(1234)
train_index <- createDataPartition(y, p = 0.5, list = FALSE)
X_train <- X[train_index, ]
y_train <- y[train_index]
X_temp <- X[-train_index, ]
y_temp <- y[-train_index]

set.seed(1234)
val_test_index <- createDataPartition(y_temp, p = 0.5, list = FALSE)
X_val <- X_temp[val_test_index, ]
y_val <- y_temp[val_test_index]
X_test <- X_temp[-val_test_index, ]
y_test <- y_temp[-val_test_index]

cat("\nDataset Sizes Before Resampling:\n")
cat("Training set size: ", dim(X_train)[1], "\n")
cat("Validation set size: ", dim(X_val)[1], "\n")
cat("Test set size: ", dim(X_test)[1], "\n")

# Merge training data into a single data frame to accommodate resampling
train_data <- data.frame(y_train, X_train)

# Balancing training data
cat("\nApplying ROSE Oversampling...\n")
train_data_rose <- ovun.sample(y_train ~ ., data = train_data, method = "both", p = 0.5, N = 1000, seed = 42)$data

# Checking data distribution after resampling
cat("\nClass Distribution After Resampling:\n")
print(table(train_data_rose$y_train))

# Re-split into x and y
X_train_rose <- train_data_rose[, -1]
y_train_rose <- train_data_rose$y_train

# K-Fold cross-validation parameters
set.seed(1234)
train_control <- trainControl(method = "cv", number = 5, verboseIter = TRUE)

```

#### Linear Regression

```{r}
library(caret)
library(pROC)
library(e1071)  # SVM
library(randomForest)
library(class)  # KNN
# Check column names after resampling and dynamically get target variable names
target_var <- colnames(train_data_rose)[1]  # Dynamically get the target variable column name

# Perform K-Fold cross-validation
cat("\nPerforming K-Fold Cross Validation on Training Set...\n")
lr_model_cv <- train(
  as.formula(paste(target_var, "~ .")),  # Dynamic construction of formulas
  data = train_data_rose,
  method = "glm",
  family = binomial,
  trControl = train_control
)

# Output K-Fold cross-validation results
cat("\nCross-Validation Results:\n")
print(lr_model_cv)

# Final model trained with K-Fold
lr_model <- lr_model_cv$finalModel

# Validation set prediction
y_val_prob <- predict(lr_model, newdata = data.frame(X_val), type = "response")
y_val_pred <- ifelse(y_val_prob > 0.5, 1, 0)

# Validation Set Performance Evaluation
confusion_matrix_val <- confusionMatrix(as.factor(y_val_pred), factor(y_val, levels = c(0, 1)))
val_accuracy <- confusion_matrix_val$overall["Accuracy"]
cat("\nValidation Set Accuracy:\n")
print(val_accuracy)

# Test Set Prediction
y_test_prob <- predict(lr_model, newdata = data.frame(X_test), type = "response")
y_test_pred <- ifelse(y_test_prob > 0.5, 1, 0)

# Test Set Performance Evaluation
confusion_matrix_test <- confusionMatrix(as.factor(y_test_pred), factor(y_test, levels = c(0, 1)))
test_accuracy <- confusion_matrix_test$overall["Accuracy"]
cat("\nTest Set Accuracy:\n")
print(test_accuracy)

# Plotting ROC curves and calculating AUC
cat("\nPlotting ROC Curve for Test Set...\n")

# Test set ROC curve
roc_test <- roc(as.numeric(y_test) - 1, y_test_prob, smooth = TRUE)  # Adding Smoothing Parameters
auc_test <- auc(roc_test)

# roc plot
plot(
  roc_test,
  col = "red",
  main = "ROC Curve for Test Set",
  print.auc = TRUE,
  legacy.axes = TRUE,
  print.auc.y = 0.4
)

# auc result
cat("\nTest Set AUC:\n")
print(auc_test)

```

#### Decision Tree Model

```{r}
# Training Decision Tree Models
cat("\nTraining Decision Tree with K-Fold Cross Validation on Resampled Data...\n")

tree_model_cv <- train(
  as.formula(paste(target_var, "~ .")), 
  data = train_data_rose,
  method = "rpart",
  trControl = train_control,
  tuneGrid = expand.grid(cp = seq(0.01, 0.1, by = 0.01))  # 调整复杂度参数
)
cat("\nDecision Tree Cross-Validation Results:\n")
print(tree_model_cv)

tree_model <- tree_model_cv$finalModel

cat("\nPlotting Decision Tree...\n")
rpart.plot(tree_model)

y_val_prob_tree <- predict(tree_model, newdata = data.frame(X_val), type = "prob")[, 2]
y_val_pred_tree <- ifelse(y_val_prob_tree > 0.5, 1, 0)

confusion_matrix_val_tree <- confusionMatrix(as.factor(y_val_pred_tree), factor(y_val, levels = c(0, 1)))
val_accuracy_tree <- confusion_matrix_val_tree$overall["Accuracy"]
cat("\nValidation Set Accuracy (Decision Tree):\n")
print(val_accuracy_tree)

y_test_prob_tree <- predict(tree_model, newdata = data.frame(X_test), type = "prob")[, 2]
y_test_pred_tree <- ifelse(y_test_prob_tree > 0.5, 1, 0)

confusion_matrix_test_tree <- confusionMatrix(as.factor(y_test_pred_tree), factor(y_test, levels = c(0, 1)))
test_accuracy_tree <- confusion_matrix_test_tree$overall["Accuracy"]
cat("\nTest Set Accuracy (Decision Tree):\n")
print(test_accuracy_tree)

roc_test_tree <- roc(as.numeric(y_test) - 1, y_test_prob_tree, smooth = TRUE)
auc_test_tree <- auc(roc_test_tree)
plot(
  roc_test_tree,
  col = "blue",
  main = "ROC Curve for Decision Tree (Resampled Data)",
  print.auc = TRUE,
  legacy.axes = TRUE,
  print.auc.y = 0.4
)
cat("\nTest Set AUC (Decision Tree):\n")
print(auc_test_tree)

```

#### Random Forest

```{r}
cat("\nTraining Random Forest with K-Fold Cross Validation...\n")
rf_model_cv <- train(
  as.formula(paste(target_var, "~ .")),
  data = train_data_rose,
  method = "rf",
  trControl = train_control,
  tuneGrid = expand.grid(mtry = sqrt(ncol(X_train_rose)))
)

cat("\nRandom Forest Cross-Validation Results:\n")
print(rf_model_cv)

rf_model <- rf_model_cv$finalModel

y_test_prob_rf <- predict(rf_model, newdata = data.frame(X_test), type = "prob")[, 2]
y_test_pred_rf <- ifelse(y_test_prob_rf > 0.5, 1, 0)

confusion_matrix_test_rf <- confusionMatrix(as.factor(y_test_pred_rf), factor(y_test, levels = c(0, 1)))
cat("\nTest Set Accuracy (Random Forest):\n")
print(confusion_matrix_test_rf$overall["Accuracy"])

roc_test_rf <- roc(as.numeric(y_test) - 1, y_test_prob_rf, smooth = TRUE)
auc_test_rf <- auc(roc_test_rf)
plot(
  roc_test_rf,
  col = "green",
  main = "ROC Curve for Random Forest",
  print.auc = TRUE,
  legacy.axes = TRUE,
  print.auc.y = 0.4
)
cat("\nTest Set AUC (Random Forest):\n")
print(auc_test_rf)
```

#### **Support Vector Classifier (SVC)**

```{r}
library(kernlab)

cat("\nTraining Support Vector Classifier (SVC) with K-Fold Cross Validation...\n")
svc_model <- ksvm(
  x = as.matrix(X_train_rose),
  y = as.factor(y_train_rose),
  type = "C-svc",
  kernel = "rbfdot",
  C = 1,
  prob.model = TRUE
)

cat("\nSVC Model Trained Successfully.\n")

# Test Set Evaluation
# Probability values for prediction categories
y_test_prob_svc <- predict(svc_model, newdata = as.matrix(X_test), type = "probabilities")[, 2]
y_test_pred_svc <- ifelse(y_test_prob_svc > 0.5, 1, 0)

confusion_matrix_test_svc <- confusionMatrix(as.factor(y_test_pred_svc), factor(y_test, levels = c(0, 1)))
cat("\nTest Set Accuracy (SVC):\n")
print(confusion_matrix_test_svc$overall["Accuracy"])

roc_test_svc <- roc(as.numeric(y_test) - 1, y_test_prob_svc, smooth = TRUE)
auc_test_svc <- auc(roc_test_svc)
plot(
  roc_test_svc,
  col = "blue",
  main = "ROC Curve for SVC",
  print.auc = TRUE,
  legacy.axes = TRUE,
  print.auc.y = 0.4
)
cat("\nTest Set AUC (SVC):\n")
print(auc_test_svc)
```

#### Gaussian Naive Bayes

```{r}
library(naivebayes)

cat("\nTraining Gaussian Naive Bayes...\n")
gnb_model_cv <- train(
  as.formula(paste(target_var, "~ .")),
  data = train_data_rose,
  method = "naive_bayes",
  trControl = train_control
)

cat("\nGaussian Naive Bayes Cross-Validation Results:\n")
print(gnb_model_cv)

gnb_model <- gnb_model_cv$finalModel

y_test_prob_gnb <- predict(gnb_model, newdata = data.frame(X_test), type = "prob")[, 2]
y_test_pred_gnb <- ifelse(y_test_prob_gnb > 0.5, 1, 0)

confusion_matrix_test_gnb <- confusionMatrix(as.factor(y_test_pred_gnb), factor(y_test, levels = c(0, 1)))
cat("\nTest Set Accuracy (Gaussian Naive Bayes):\n")
print(confusion_matrix_test_gnb$overall["Accuracy"])

roc_test_gnb <- roc(as.numeric(y_test) - 1, y_test_prob_gnb, smooth = TRUE)
auc_test_gnb <- auc(roc_test_gnb)
plot(
  roc_test_gnb,
  col = "purple",
  main = "ROC Curve for Gaussian Naive Bayes",
  print.auc = TRUE,
  legacy.axes = TRUE,
  print.auc.y = 0.4
)
cat("\nTest Set AUC (Gaussian Naive Bayes):\n")
print(auc_test_gnb)

```

#### K Nearest Neighbor

```{r}
cat("\nTraining K-Nearest Neighbor (KNN) with K-Fold Cross Validation...\n")
knn_model_cv <- train(
  as.formula(paste(target_var, "~ .")),
  data = train_data_rose,
  method = "knn",
  trControl = train_control,
  tuneGrid = expand.grid(k = seq(3, 15, by = 2))
)

cat("\nKNN Cross-Validation Results:\n")
print(knn_model_cv)

knn_model <- knn_model_cv$finalModel

y_test_prob_knn <- predict(knn_model, newdata = data.frame(X_test), type = "prob")[, 2]
y_test_pred_knn <- ifelse(y_test_prob_knn > 0.5, 1, 0)

confusion_matrix_test_knn <- confusionMatrix(as.factor(y_test_pred_knn), factor(y_test, levels = c(0, 1)))
cat("\nTest Set Accuracy (KNN):\n")
print(confusion_matrix_test_knn$overall["Accuracy"])

roc_test_knn <- roc(as.numeric(y_test) - 1, y_test_prob_knn, smooth = TRUE)
auc_test_knn <- auc(roc_test_knn)
plot(
  roc_test_knn,
  col = "orange",
  main = "ROC Curve for KNN",
  print.auc = TRUE,
  legacy.axes = TRUE,
  print.auc.y = 0.4
)
cat("\nTest Set AUC (KNN):\n")
print(auc_test_knn)
```

#### XGBoost

```{r}
cat("\nTraining XGBoost with K-Fold Cross Validation...\n")
xgb_model_cv <- train(
  as.formula(paste(target_var, "~ .")),
  data = train_data_rose,
  method = "xgbTree",
  trControl = train_control,
  tuneGrid = expand.grid(nrounds = 100, max_depth = 6, eta = 0.3, gamma = 0, colsample_bytree = 1, min_child_weight = 1, subsample = 1)
)

cat("\nXGBoost Cross-Validation Results:\n")
print(xgb_model_cv)

xgb_model <- xgb_model_cv$finalModel

# Test Set Evaluation
# Return the probability vector
y_test_prob_xgb <- predict(xgb_model, newdata = as.matrix(X_test))  
# II. Forecasts by category
y_test_pred_xgb <- ifelse(y_test_prob_xgb > 0.5, 1, 0)  

# Test Set Performance Evaluation
confusion_matrix_test_xgb <- confusionMatrix(as.factor(y_test_pred_xgb), factor(y_test, levels = c(0, 1)))
cat("\nTest Set Accuracy (XGBoost):\n")
print(confusion_matrix_test_xgb$overall["Accuracy"])

roc_test_xgb <- roc(as.numeric(y_test) - 1, y_test_prob_xgb, smooth = TRUE)
auc_test_xgb <- auc(roc_test_xgb)
plot(
  roc_test_xgb,
  col = "red",
  main = "ROC Curve for XGBoost",
  print.auc = TRUE,
  legacy.axes = TRUE,
  print.auc.y = 0.4
)
cat("\nTest Set AUC (XGBoost):\n")
print(auc_test_xgb)
```

#### ROC Curve

```{r}
library(pROC)
library(ggplot2)

# Collect ROC objects for all models
roc_list <- list(
  # Logistic Regression ROC
  "Logistic Regression" = roc_test,
  # Decision Tree ROC
  "Decision Tree" = roc_test_tree,
  # Random Forest ROC
  "Random Forest" = roc_test_rf,
  # Support Vector Classifier ROC
  "SVC" = roc_test_svc,
  # Gaussian Naive Bayes ROC
  "Gaussian Naive Bayes" = roc_test_gnb,
  # K-Nearest Neighbor ROC
  "KNN" = roc_test_knn,
  # XGBoost ROC
  "XGBoost" = roc_test_xgb
)

# Converting ROC objects to dataframe format
roc_data <- lapply(names(roc_list), function(model_name) {
  roc_obj <- roc_list[[model_name]]
  data.frame(
    FPR = 1 - roc_obj$specificities,  # False Positive Rate
    TPR = roc_obj$sensitivities,      # True Positive Rate
    Model = model_name
  )
})

# Merge all model data
roc_data <- do.call(rbind, roc_data) 

# Plotting ROC curves for multiple models
ggplot(roc_data, aes(x = FPR, y = TPR, color = Model)) +
  geom_line(size = 0.5) +
  scale_color_manual(
    values = c(
      "Logistic Regression" = "red",
      "Decision Tree" = "blue",
      "Random Forest" = "green",
      "SVC" = "purple",
      "Gaussian Naive Bayes" = "yellow",
      "KNN" = "orange",
      "XGBoost" = "cyan"
    )
  ) +
  labs(
    title = "ROC Curves for Multiple Models",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")

```

## Results {#sec-results}

Initial Data Cleaning:

The dataset contained 309 rows and 16 columns. After removing 33 duplicate entries, the dataset was reduced to 276 unique records.There were no missing values in the dataset, ensuring complete data for analysis.

Data Transformation:

Non-numeric features, such as GENDER and LUNG_CANCER, were encoded into numerical values using LabelEncoder. Binary variables (e.g., YES/NO) were transformed into 1 and 0 for consistency.

Exploratory Data Analysis:

The target variable LUNG_CANCER was imbalanced with 238 positive cases (86.23%) and 38 negative cases (13.77%). Various features, such as GENDER, AGE, SMOKING, and SHORTNESS OF BREATH, showed weak or no significant relationship with the target variable and were dropped to simplify the dataset.

Correlation Analysis:

A correlation matrix was created to identify interdependencies among features.Significant correlations were observed between features like ANXIETY and YELLOW_FINGERS (r \> 0.50), leading to the creation of a new combined feature, ANXYELFIN.

Feature Selection:

The newly created feature ANXYELFIN was introduced to capture combined effects of ANXIETY and YELLOW_FINGERS. And the final dataset consisted of 13 columns, including the engineered feature and target variable.

Resampling of training data:

In order to achieve data balance during the data processing in this paper, the training set was resampled in both directions using the ROSE package, i.e., both oversampling and undersampling were performed to balance the category distribution. The resampling parameter p = 0.5 and the total sample size N = 1000 were set to ensure a more balanced category distribution. Output the resampled category distribution: Class Distribution After Resampling. 0: X 1: Y Reclassify the resampled data into independent variables (X_train_rose) and dependent variables (y_train_rose) to prepare for model training. In the cross-validation setup, the training process was evaluated using the K-Fold Cross-Validation) method. A 5-fold cross-validation was configured and verboseIter was enabled to monitor the training status for each iteration.

In cross-validation, XGBoost and Random Forest achieved the highest cross-validation accuracies (98%), indicating strong performance during training. KNN with k=3 also performed very well (97.2% accuracy). In validation set, poor validation accuracies across most models suggest potential overfitting or issues with the validation split. In test set, XGBoost showed the best test accuracy (88.23%) and reasonable AUC (0.8703), demonstrating good generalization. The Decision Tree model achieved the highest AUC (0.9397), despite lower accuracy (13.23%). In conclusion, XGBoost and Random Forest models are the strongest performers based on cross-validation and test set metrics. Poor validation and test accuracies in other models highlight possible challenges in model generalization or dataset partitioning. Additional fine-tuning and adjustments, particularly for validation strategies, could further enhance performance.

## Conclusion

This project successfully demonstrated the application of machine learning for lung cancer prediction using structured patient data. Through data cleaning, transformation, feature engineering, and robust model evaluation, this analysis highlights the potential of predictive modeling to support early diagnosis and improve clinical efficiency. High data quality was ensured through rigorous data preprocessing steps, such as de-weighting, coding categorical variables, and balancing category distributions through resampling, while target variable imbalance was effectively addressed. Based on model performance, the XGBoost and Random Forest models performed the best with a cross-validation accuracy of 98% and demonstrated good performance on the test set. Decision trees, although less accurate, achieved the highest AUC (0.9397) on the test set, suggesting an advantage in specific application scenarios. The poor performance of most models on the validation set reflects possible overfitting problems or deficiencies in the validation set partitioning strategy. Further optimization of the validation method can help improve the generalization ability of the models.

In summary, this study demonstrates the potential application of machine learning in the early diagnosis of lung cancer and suggests that the performance can be further improved by fine optimization of the model and data processing process to provide strong auxiliary support for clinical diagnosis.
